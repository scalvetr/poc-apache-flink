apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: simple-batch-job
spec:
  image: localhost:5001/simple-batch-job:latest
  imagePullPolicy: Always
  flinkVersion: v1_17
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: "2"
    # configure S3
    # https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html
    #s3.access.key: "IFRVAFXyBPZFdUOPPE7U"
    #s3.secret.key: "JhP251bDsOQHe8ZNM1iEBIhzu2o9pYKTwsokpeCu"
    s3.access.key: "minio"
    s3.secret.key: "minio123"
    s3.endpoint: "http://myminio-hl.minio-tenant:9000"
    #s3.path.style.access: "true"
    #s3.proxy.host: ""
    #s3.proxy.port: ""
  serviceAccount: flink
  ingress:
    template: "{{name}}.{{namespace}}.localtest.me"
    className: "nginx"
    #annotations:
    #  nginx.ingress.kubernetes.io/rewrite-target: "/$2"
  jobManager:
    resource:
      memory: "2048m"
      cpu: 1
  taskManager:
    resource:
      memory: "2048m"
      cpu: 1
  job:
    jarURI: local:///opt/flink/usrlib/flink-job.jar
    parallelism: 2
    upgradeMode: stateless
    args:
      - "--input"
      - "s3://demo-bucket/in/largefile.txt"
      - "--output"
      - "s3://demo-bucket/out/largefile.csv"
